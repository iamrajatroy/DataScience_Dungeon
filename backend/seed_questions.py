# ========================================
# DATA SCIENCE DUNGEON - SEED QUESTIONS
# ========================================
"""
Script to seed the database with data science questions.
Run this after starting the server to populate the questions table.
"""

from database import SessionLocal, engine, Base
from models import Question

# Create tables if they don't exist
Base.metadata.create_all(bind=engine)

# Question bank - 130 data science questions
QUESTIONS = [
    # ==================== STATISTICS ====================
    # Easy
    {"topic": "Statistics", "difficulty": "easy", "question_text": "What does the mean of a dataset represent?", "option_a": "The most frequent value", "option_b": "The middle value when sorted", "option_c": "The average of all values", "option_d": "The range of values", "correct_answer": "C", "explanation": "The mean is calculated by summing all values and dividing by the count, representing the average."},
    {"topic": "Statistics", "difficulty": "easy", "question_text": "What is the median of the dataset [1, 3, 5, 7, 9]?", "option_a": "3", "option_b": "5", "option_c": "7", "option_d": "25", "correct_answer": "B", "explanation": "The median is the middle value when data is sorted. In [1,3,5,7,9], the median is 5."},
    {"topic": "Statistics", "difficulty": "easy", "question_text": "What is standard deviation used to measure?", "option_a": "Central tendency", "option_b": "Data spread/dispersion", "option_c": "Data symmetry", "option_d": "Sample size", "correct_answer": "B", "explanation": "Standard deviation measures how spread out the values are from the mean."},
    {"topic": "Statistics", "difficulty": "easy", "question_text": "Which measure of central tendency is most affected by outliers?", "option_a": "Mean", "option_b": "Median", "option_c": "Mode", "option_d": "Range", "correct_answer": "A", "explanation": "The mean is sensitive to extreme values (outliers) because it uses all data points."},
    {"topic": "Statistics", "difficulty": "easy", "question_text": "What is a probability value that represents impossibility?", "option_a": "0", "option_b": "0.5", "option_c": "1", "option_d": "-1", "correct_answer": "A", "explanation": "A probability of 0 means the event is impossible, while 1 means certain."},
    {"topic": "Statistics", "difficulty": "easy", "question_text": "What type of variable can take only specific values?", "option_a": "Continuous", "option_b": "Discrete", "option_c": "Ordinal", "option_d": "Nominal", "correct_answer": "B", "explanation": "Discrete variables can only take specific, countable values (like integers)."},
    {"topic": "Statistics", "difficulty": "easy", "question_text": "What chart is best for showing the distribution of a single continuous variable?", "option_a": "Bar chart", "option_b": "Pie chart", "option_c": "Histogram", "option_d": "Line chart", "correct_answer": "C", "explanation": "Histograms show frequency distributions of continuous data by grouping into bins."},
    
    # Medium
    {"topic": "Statistics", "difficulty": "medium", "question_text": "What does a p-value of 0.03 indicate in hypothesis testing?", "option_a": "3% chance the null hypothesis is true", "option_b": "3% probability of observing results if null is true", "option_c": "97% confidence level", "option_d": "The test failed", "correct_answer": "B", "explanation": "P-value represents the probability of observing results at least as extreme as those obtained, assuming the null hypothesis is true."},
    {"topic": "Statistics", "difficulty": "medium", "question_text": "What is the Central Limit Theorem about?", "option_a": "Sample means are always normal", "option_b": "Large sample means approach normal distribution", "option_c": "Population must be normal", "option_d": "Variance equals mean", "correct_answer": "B", "explanation": "CLT states that the sampling distribution of means approaches normal distribution as sample size increases, regardless of population distribution."},
    {"topic": "Statistics", "difficulty": "medium", "question_text": "What is a Type I error?", "option_a": "Failing to reject a false null hypothesis", "option_b": "Rejecting a true null hypothesis", "option_c": "Accepting an alternative hypothesis", "option_d": "Using wrong sample size", "correct_answer": "B", "explanation": "Type I error (false positive) occurs when we reject a null hypothesis that is actually true."},
    {"topic": "Statistics", "difficulty": "medium", "question_text": "What does correlation coefficient r = -0.95 indicate?", "option_a": "Strong positive relationship", "option_b": "Weak negative relationship", "option_c": "Strong negative relationship", "option_d": "No relationship", "correct_answer": "C", "explanation": "A correlation close to -1 indicates a strong negative linear relationship between variables."},
    {"topic": "Statistics", "difficulty": "medium", "question_text": "What is the purpose of a confidence interval?", "option_a": "To find exact parameter values", "option_b": "To estimate a range for population parameters", "option_c": "To test hypotheses", "option_d": "To calculate variance", "correct_answer": "B", "explanation": "Confidence intervals provide a range of values likely to contain the true population parameter."},
    {"topic": "Statistics", "difficulty": "medium", "question_text": "What distribution is used for rare events?", "option_a": "Normal", "option_b": "Binomial", "option_c": "Poisson", "option_d": "Uniform", "correct_answer": "C", "explanation": "Poisson distribution models the probability of rare events occurring in a fixed interval."},
    {"topic": "Statistics", "difficulty": "medium", "question_text": "What is multicollinearity in regression?", "option_a": "Too many dependent variables", "option_b": "High correlation between predictors", "option_c": "Non-linear relationships", "option_d": "Heteroscedasticity", "correct_answer": "B", "explanation": "Multicollinearity occurs when independent variables are highly correlated with each other."},
    
    # Hard
    {"topic": "Statistics", "difficulty": "hard", "question_text": "What is the difference between MLE and MAP estimation?", "option_a": "MLE uses priors, MAP doesn't", "option_b": "MAP uses priors, MLE doesn't", "option_c": "They are identical", "option_d": "MAP is for classification only", "correct_answer": "B", "explanation": "Maximum A Posteriori (MAP) incorporates prior beliefs while Maximum Likelihood (MLE) only uses data likelihood."},
    {"topic": "Statistics", "difficulty": "hard", "question_text": "What does the Kolmogorov-Smirnov test evaluate?", "option_a": "Mean differences", "option_b": "Variance equality", "option_c": "Distribution fit", "option_d": "Correlation strength", "correct_answer": "C", "explanation": "K-S test compares a sample distribution to a reference distribution or compares two samples."},
    {"topic": "Statistics", "difficulty": "hard", "question_text": "What is heteroscedasticity?", "option_a": "Constant variance in residuals", "option_b": "Non-constant variance in residuals", "option_c": "Correlated errors", "option_d": "Non-normal errors", "correct_answer": "B", "explanation": "Heteroscedasticity refers to non-constant variance of residuals across levels of predictors."},
    {"topic": "Statistics", "difficulty": "hard", "question_text": "What is the Bonferroni correction used for?", "option_a": "Adjusting p-values for multiple comparisons", "option_b": "Correcting sampling bias", "option_c": "Normalizing data", "option_d": "Imputing missing values", "correct_answer": "A", "explanation": "Bonferroni correction adjusts significance levels when performing multiple statistical tests."},
    
    # ==================== MACHINE LEARNING ====================
    # Easy
    {"topic": "Machine Learning", "difficulty": "easy", "question_text": "What is supervised learning?", "option_a": "Learning without labels", "option_b": "Learning with labeled data", "option_c": "Learning by trial and error", "option_d": "Learning from rules", "correct_answer": "B", "explanation": "Supervised learning uses labeled training data where both inputs and correct outputs are provided."},
    {"topic": "Machine Learning", "difficulty": "easy", "question_text": "What is overfitting?", "option_a": "Model performs well on test data", "option_b": "Model is too simple for the data", "option_c": "Model memorizes training data, fails on new data", "option_d": "Model has too few parameters", "correct_answer": "C", "explanation": "Overfitting occurs when a model learns noise in training data and fails to generalize."},
    {"topic": "Machine Learning", "difficulty": "easy", "question_text": "Which algorithm is used for classification?", "option_a": "Linear Regression", "option_b": "K-Means", "option_c": "Decision Tree", "option_d": "PCA", "correct_answer": "C", "explanation": "Decision Trees can be used for classification by splitting data based on features."},
    {"topic": "Machine Learning", "difficulty": "easy", "question_text": "What does K-Means algorithm do?", "option_a": "Classifies data", "option_b": "Clusters data into K groups", "option_c": "Reduces dimensions", "option_d": "Predicts values", "correct_answer": "B", "explanation": "K-Means is an unsupervised algorithm that partitions data into K clusters."},
    {"topic": "Machine Learning", "difficulty": "easy", "question_text": "What is a feature in machine learning?", "option_a": "The target variable", "option_b": "An input variable used for prediction", "option_c": "The model output", "option_d": "A hyperparameter", "correct_answer": "B", "explanation": "Features are input variables (attributes) used by models to make predictions."},
    {"topic": "Machine Learning", "difficulty": "easy", "question_text": "What is the purpose of train-test split?", "option_a": "To speed up training", "option_b": "To evaluate model on unseen data", "option_c": "To reduce overfitting", "option_d": "To increase accuracy", "correct_answer": "B", "explanation": "Train-test split reserves data for evaluating how well the model generalizes to new data."},
    {"topic": "Machine Learning", "difficulty": "easy", "question_text": "What is a label in supervised learning?", "option_a": "Input data", "option_b": "The correct answer/output", "option_c": "Feature name", "option_d": "Model parameter", "correct_answer": "B", "explanation": "Labels are the target values that the model learns to predict."},
    
    # Medium
    {"topic": "Machine Learning", "difficulty": "medium", "question_text": "What is cross-validation used for?", "option_a": "Training models faster", "option_b": "Robust performance estimation", "option_c": "Feature selection", "option_d": "Data augmentation", "correct_answer": "B", "explanation": "Cross-validation provides a more robust estimate of model performance by training on multiple data splits."},
    {"topic": "Machine Learning", "difficulty": "medium", "question_text": "What does the bias-variance tradeoff describe?", "option_a": "Speed vs accuracy", "option_b": "Model complexity vs generalization", "option_c": "Training time vs epochs", "option_d": "Features vs samples", "correct_answer": "B", "explanation": "Bias-variance tradeoff describes balancing model complexity: too simple (high bias) vs too complex (high variance)."},
    {"topic": "Machine Learning", "difficulty": "medium", "question_text": "What is the purpose of regularization?", "option_a": "To speed up training", "option_b": "To prevent overfitting", "option_c": "To increase model complexity", "option_d": "To handle missing data", "correct_answer": "B", "explanation": "Regularization adds a penalty term to prevent overfitting by discouraging complex models."},
    {"topic": "Machine Learning", "difficulty": "medium", "question_text": "What is the difference between L1 and L2 regularization?", "option_a": "L1 uses squared weights, L2 uses absolute", "option_b": "L1 uses absolute weights, L2 uses squared", "option_c": "They are the same", "option_d": "L1 is for classification only", "correct_answer": "B", "explanation": "L1 (Lasso) uses absolute weights and can zero out features; L2 (Ridge) uses squared weights."},
    {"topic": "Machine Learning", "difficulty": "medium", "question_text": "What is precision in classification?", "option_a": "TP / (TP + FN)", "option_b": "TP / (TP + FP)", "option_c": "TN / (TN + FP)", "option_d": "(TP + TN) / Total", "correct_answer": "B", "explanation": "Precision = True Positives / (True Positives + False Positives), measuring prediction accuracy for positive class."},
    {"topic": "Machine Learning", "difficulty": "medium", "question_text": "What is the purpose of feature scaling?", "option_a": "To remove features", "option_b": "To normalize feature ranges", "option_c": "To add new features", "option_d": "To handle missing values", "correct_answer": "B", "explanation": "Feature scaling normalizes features to similar ranges, improving algorithm performance."},
    {"topic": "Machine Learning", "difficulty": "medium", "question_text": "What is ensemble learning?", "option_a": "Using a single strong model", "option_b": "Combining multiple models", "option_c": "Training on subsets", "option_d": "Using neural networks", "correct_answer": "B", "explanation": "Ensemble learning combines multiple models to improve prediction accuracy and robustness."},
    {"topic": "Machine Learning", "difficulty": "medium", "question_text": "What does the ROC curve plot?", "option_a": "Precision vs Recall", "option_b": "True Positive Rate vs False Positive Rate", "option_c": "Accuracy vs Loss", "option_d": "Bias vs Variance", "correct_answer": "B", "explanation": "ROC curve plots True Positive Rate (sensitivity) against False Positive Rate at various thresholds."},
    
    # Hard
    {"topic": "Machine Learning", "difficulty": "hard", "question_text": "What is the kernel trick in SVM?", "option_a": "Reducing dimensions", "option_b": "Computing similarities in high-dim space efficiently", "option_c": "Speeding up training", "option_d": "Regularization technique", "correct_answer": "B", "explanation": "The kernel trick allows SVMs to compute high-dimensional feature space similarities without explicit transformation."},
    {"topic": "Machine Learning", "difficulty": "hard", "question_text": "What is gradient boosting's main idea?", "option_a": "Training models in parallel", "option_b": "Sequential training on residuals", "option_c": "Random feature selection", "option_d": "Bagging with replacement", "correct_answer": "B", "explanation": "Gradient boosting trains models sequentially, with each model correcting errors of previous ones."},
    {"topic": "Machine Learning", "difficulty": "hard", "question_text": "What causes the curse of dimensionality?", "option_a": "Too few features", "option_b": "Data becomes sparse in high dimensions", "option_c": "Too many samples", "option_d": "Linear relationships", "correct_answer": "B", "explanation": "In high dimensions, data becomes sparse and distances become less meaningful, affecting algorithms."},
    {"topic": "Machine Learning", "difficulty": "hard", "question_text": "What is SMOTE used for?", "option_a": "Feature extraction", "option_b": "Handling class imbalance", "option_c": "Dimensionality reduction", "option_d": "Model selection", "correct_answer": "B", "explanation": "SMOTE (Synthetic Minority Over-sampling Technique) creates synthetic samples for minority classes."},
    
    # ==================== DEEP LEARNING ====================
    # Easy
    {"topic": "Deep Learning", "difficulty": "easy", "question_text": "What is a neural network?", "option_a": "A statistical test", "option_b": "Layers of connected nodes inspired by the brain", "option_c": "A database system", "option_d": "A clustering algorithm", "correct_answer": "B", "explanation": "Neural networks are computing systems with interconnected nodes organized in layers, inspired by biological neurons."},
    {"topic": "Deep Learning", "difficulty": "easy", "question_text": "What is an activation function?", "option_a": "Function to start training", "option_b": "Function that introduces non-linearity", "option_c": "Function to load data", "option_d": "Function to save models", "correct_answer": "B", "explanation": "Activation functions add non-linearity, allowing networks to learn complex patterns."},
    {"topic": "Deep Learning", "difficulty": "easy", "question_text": "What is backpropagation?", "option_a": "Forward pass through network", "option_b": "Algorithm to compute gradients", "option_c": "Data preprocessing step", "option_d": "Model evaluation", "correct_answer": "B", "explanation": "Backpropagation calculates gradients of the loss with respect to weights for optimization."},
    {"topic": "Deep Learning", "difficulty": "easy", "question_text": "What does CNN stand for?", "option_a": "Central Neural Network", "option_b": "Convolutional Neural Network", "option_c": "Connected Node Network", "option_d": "Computed Numeric Network", "correct_answer": "B", "explanation": "CNN stands for Convolutional Neural Network, commonly used for image processing."},
    {"topic": "Deep Learning", "difficulty": "easy", "question_text": "What is an epoch in training?", "option_a": "One pass through entire training data", "option_b": "One batch update", "option_c": "Model evaluation step", "option_d": "Initial weight setting", "correct_answer": "A", "explanation": "An epoch is one complete pass through the entire training dataset."},
    {"topic": "Deep Learning", "difficulty": "easy", "question_text": "What is the purpose of dropout?", "option_a": "Speed up training", "option_b": "Prevent overfitting", "option_c": "Add more layers", "option_d": "Increase accuracy", "correct_answer": "B", "explanation": "Dropout randomly deactivates neurons during training to prevent overfitting."},
    
    # Medium
    {"topic": "Deep Learning", "difficulty": "medium", "question_text": "What is vanishing gradient problem?", "option_a": "Gradients become too large", "option_b": "Gradients become too small to update weights", "option_c": "Gradient computation is slow", "option_d": "Gradients oscillate", "correct_answer": "B", "explanation": "Vanishing gradients occur when gradients become extremely small, preventing effective learning in deep networks."},
    {"topic": "Deep Learning", "difficulty": "medium", "question_text": "What is the purpose of batch normalization?", "option_a": "To increase batch size", "option_b": "To normalize layer inputs for faster training", "option_c": "To reduce parameters", "option_d": "To add noise", "correct_answer": "B", "explanation": "Batch normalization normalizes layer inputs, reducing internal covariate shift and accelerating training."},
    {"topic": "Deep Learning", "difficulty": "medium", "question_text": "What is transfer learning?", "option_a": "Moving data between systems", "option_b": "Using pre-trained models for new tasks", "option_c": "Copying model weights", "option_d": "Data augmentation", "correct_answer": "B", "explanation": "Transfer learning uses knowledge from pre-trained models to solve new, related problems."},
    {"topic": "Deep Learning", "difficulty": "medium", "question_text": "What is an LSTM used for?", "option_a": "Image classification", "option_b": "Sequential/time-series data", "option_c": "Dimensionality reduction", "option_d": "Clustering", "correct_answer": "B", "explanation": "LSTM (Long Short-Term Memory) networks handle sequential data by maintaining long-term dependencies."},
    {"topic": "Deep Learning", "difficulty": "medium", "question_text": "What optimizer is commonly used in deep learning?", "option_a": "K-Means", "option_b": "Adam", "option_c": "PCA", "option_d": "DBSCAN", "correct_answer": "B", "explanation": "Adam (Adaptive Moment Estimation) is a popular optimizer combining momentum and adaptive learning rates."},
    {"topic": "Deep Learning", "difficulty": "medium", "question_text": "What is the purpose of padding in CNNs?", "option_a": "To increase speed", "option_b": "To preserve spatial dimensions", "option_c": "To reduce parameters", "option_d": "To add noise", "correct_answer": "B", "explanation": "Padding adds borders to input to control output size and preserve edge information."},
    {"topic": "Deep Learning", "difficulty": "medium", "question_text": "What is early stopping?", "option_a": "Stopping training when validation loss stops improving", "option_b": "Reducing learning rate", "option_c": "Removing layers", "option_d": "Batch normalization", "correct_answer": "A", "explanation": "Early stopping prevents overfitting by stopping training when validation performance degrades."},
    
    # Hard
    {"topic": "Deep Learning", "difficulty": "hard", "question_text": "What is the attention mechanism?", "option_a": "Focusing on relevant parts of input", "option_b": "Increasing model depth", "option_c": "Reducing computation", "option_d": "Data preprocessing", "correct_answer": "A", "explanation": "Attention allows models to focus on relevant parts of input when making predictions."},
    {"topic": "Deep Learning", "difficulty": "hard", "question_text": "What is residual learning (ResNet)?", "option_a": "Learning absolute values", "option_b": "Learning residual mappings with skip connections", "option_c": "Reducing network depth", "option_d": "Dropout variant", "correct_answer": "B", "explanation": "ResNet uses skip connections to learn residual functions, enabling very deep networks."},
    {"topic": "Deep Learning", "difficulty": "hard", "question_text": "What is weight initialization's purpose?", "option_a": "To speed up inference", "option_b": "To prevent vanishing/exploding gradients", "option_c": "To reduce memory", "option_d": "To simplify architecture", "correct_answer": "B", "explanation": "Proper weight initialization helps maintain gradient flow and stable training."},
    {"topic": "Deep Learning", "difficulty": "hard", "question_text": "What is the receptive field in CNNs?", "option_a": "Number of filters", "option_b": "Input region each neuron 'sees'", "option_c": "Output image size", "option_d": "Pooling window", "correct_answer": "B", "explanation": "Receptive field is the region of input that influences a particular output unit's activation."},
    
    # ==================== REINFORCEMENT LEARNING ====================
    # Easy
    {"topic": "Reinforcement Learning", "difficulty": "easy", "question_text": "What is reinforcement learning?", "option_a": "Learning from labeled data", "option_b": "Learning from rewards/penalties", "option_c": "Learning from clusters", "option_d": "Learning from rules", "correct_answer": "B", "explanation": "RL involves an agent learning to make decisions by receiving rewards or penalties."},
    {"topic": "Reinforcement Learning", "difficulty": "easy", "question_text": "What is an agent in RL?", "option_a": "The environment", "option_b": "The learner that takes actions", "option_c": "The reward signal", "option_d": "The dataset", "correct_answer": "B", "explanation": "The agent is the learner that observes states, takes actions, and receives rewards."},
    {"topic": "Reinforcement Learning", "difficulty": "easy", "question_text": "What is a reward in RL?", "option_a": "Model parameters", "option_b": "Feedback signal for actions", "option_c": "Training data", "option_d": "Network architecture", "correct_answer": "B", "explanation": "Rewards are scalar feedback signals that indicate how good an action was."},
    {"topic": "Reinforcement Learning", "difficulty": "easy", "question_text": "What is the environment in RL?", "option_a": "The agent's memory", "option_b": "What the agent interacts with", "option_c": "The training data", "option_d": "The neural network", "correct_answer": "B", "explanation": "The environment is the world the agent interacts with and receives observations from."},
    
    # Medium
    {"topic": "Reinforcement Learning", "difficulty": "medium", "question_text": "What is the exploration-exploitation tradeoff?", "option_a": "Speed vs accuracy", "option_b": "Trying new actions vs using known good actions", "option_c": "Training vs testing", "option_d": "Online vs offline", "correct_answer": "B", "explanation": "Balancing between exploring new actions to find better rewards vs exploiting known good actions."},
    {"topic": "Reinforcement Learning", "difficulty": "medium", "question_text": "What is Q-learning?", "option_a": "Supervised learning method", "option_b": "Model-free RL algorithm learning action values", "option_c": "Clustering algorithm", "option_d": "Dimensionality reduction", "correct_answer": "B", "explanation": "Q-learning is a model-free RL algorithm that learns the value of actions in states."},
    {"topic": "Reinforcement Learning", "difficulty": "medium", "question_text": "What is the discount factor (gamma) in RL?", "option_a": "Learning rate", "option_b": "Factor for valuing future rewards", "option_c": "Regularization term", "option_d": "Batch size", "correct_answer": "B", "explanation": "Discount factor determines how much future rewards are valued compared to immediate rewards."},
    {"topic": "Reinforcement Learning", "difficulty": "medium", "question_text": "What is a policy in RL?", "option_a": "Network architecture", "option_b": "Mapping from states to actions", "option_c": "Reward function", "option_d": "Training algorithm", "correct_answer": "B", "explanation": "A policy defines the agent's behavior by mapping states to actions (or action probabilities)."},
    {"topic": "Reinforcement Learning", "difficulty": "medium", "question_text": "What is the Markov property?", "option_a": "Future depends on entire history", "option_b": "Future depends only on current state", "option_c": "Actions are random", "option_d": "Rewards are constant", "correct_answer": "B", "explanation": "Markov property states that the future is independent of the past given the present state."},
    
    # Hard
    {"topic": "Reinforcement Learning", "difficulty": "hard", "question_text": "What is the Bellman equation used for?", "option_a": "Defining optimal value functions recursively", "option_b": "Computing gradients", "option_c": "Feature extraction", "option_d": "Sampling actions", "correct_answer": "A", "explanation": "Bellman equations define value functions recursively, foundational to many RL algorithms."},
    {"topic": "Reinforcement Learning", "difficulty": "hard", "question_text": "What is actor-critic architecture?", "option_a": "Single network for value estimation", "option_b": "Separate networks for policy and value", "option_c": "Ensemble of policies", "option_d": "Model-based approach", "correct_answer": "B", "explanation": "Actor-critic uses separate networks: actor for policy, critic for value estimation."},
    {"topic": "Reinforcement Learning", "difficulty": "hard", "question_text": "What is PPO's main innovation?", "option_a": "Using value functions only", "option_b": "Clipped surrogate objective for stable updates", "option_c": "Off-policy learning", "option_d": "Model-based planning", "correct_answer": "B", "explanation": "PPO (Proximal Policy Optimization) uses clipped objectives to prevent large policy updates."},
    {"topic": "Reinforcement Learning", "difficulty": "hard", "question_text": "What is experience replay?", "option_a": "Replaying the game", "option_b": "Storing and sampling past transitions", "option_c": "Resetting environment", "option_d": "Copying policies", "correct_answer": "B", "explanation": "Experience replay stores past experiences in a buffer and samples them for training."},
    
    # ==================== NLP ====================
    # Easy
    {"topic": "NLP", "difficulty": "easy", "question_text": "What does NLP stand for?", "option_a": "Neural Learning Process", "option_b": "Natural Language Processing", "option_c": "Network Layer Protocol", "option_d": "Numeric Linear Programming", "correct_answer": "B", "explanation": "NLP stands for Natural Language Processing, dealing with text and language understanding."},
    {"topic": "NLP", "difficulty": "easy", "question_text": "What is tokenization?", "option_a": "Encrypting text", "option_b": "Splitting text into smaller units", "option_c": "Translating text", "option_d": "Summarizing text", "correct_answer": "B", "explanation": "Tokenization splits text into tokens (words, subwords, or characters) for processing."},
    {"topic": "NLP", "difficulty": "easy", "question_text": "What is a stop word?", "option_a": "Error in text", "option_b": "Common words often filtered out", "option_c": "Punctuation", "option_d": "Rare words", "correct_answer": "B", "explanation": "Stop words are common words (the, is, a) often removed as they carry little meaning."},
    {"topic": "NLP", "difficulty": "easy", "question_text": "What is stemming?", "option_a": "Adding prefixes", "option_b": "Reducing words to their root form", "option_c": "Translating words", "option_d": "Finding synonyms", "correct_answer": "B", "explanation": "Stemming reduces words to their root/base form (running → run)."},
    {"topic": "NLP", "difficulty": "easy", "question_text": "What is sentiment analysis?", "option_a": "Grammar checking", "option_b": "Determining emotional tone of text", "option_c": "Language translation", "option_d": "Text summarization", "correct_answer": "B", "explanation": "Sentiment analysis determines whether text expresses positive, negative, or neutral sentiment."},
    
    # Medium
    {"topic": "NLP", "difficulty": "medium", "question_text": "What is TF-IDF?", "option_a": "Neural network architecture", "option_b": "Term frequency-inverse document frequency weighting", "option_c": "Translation function", "option_d": "Text formatting", "correct_answer": "B", "explanation": "TF-IDF weights words by their frequency in a document relative to their corpus frequency."},
    {"topic": "NLP", "difficulty": "medium", "question_text": "What are word embeddings?", "option_a": "Word counts", "option_b": "Dense vector representations of words", "option_c": "Grammar rules", "option_d": "Stop word lists", "correct_answer": "B", "explanation": "Word embeddings represent words as dense vectors where similar words are close in vector space."},
    {"topic": "NLP", "difficulty": "medium", "question_text": "What is Word2Vec?", "option_a": "Translation model", "option_b": "Algorithm to learn word embeddings", "option_c": "Text classifier", "option_d": "Tokenizer", "correct_answer": "B", "explanation": "Word2Vec is an algorithm that learns word embeddings from large text corpora."},
    {"topic": "NLP", "difficulty": "medium", "question_text": "What is named entity recognition (NER)?", "option_a": "Finding verbs", "option_b": "Identifying entities like names, places, dates", "option_c": "Grammar checking", "option_d": "Sentiment analysis", "correct_answer": "B", "explanation": "NER identifies and classifies named entities (persons, organizations, locations) in text."},
    {"topic": "NLP", "difficulty": "medium", "question_text": "What is the bag-of-words model?", "option_a": "Neural network", "option_b": "Text representation ignoring word order", "option_c": "Grammar parser", "option_d": "Translation model", "correct_answer": "B", "explanation": "Bag-of-words represents text as word frequency counts, ignoring grammar and word order."},
    {"topic": "NLP", "difficulty": "medium", "question_text": "What is lemmatization?", "option_a": "Random word selection", "option_b": "Reducing words to dictionary form", "option_c": "Text encryption", "option_d": "Adding punctuation", "correct_answer": "B", "explanation": "Lemmatization reduces words to their dictionary form (better → good) using vocabulary and morphology."},
    
    # Hard
    {"topic": "NLP", "difficulty": "hard", "question_text": "What is the attention mechanism in NLP?", "option_a": "Filtering stop words", "option_b": "Weighting input parts by relevance", "option_c": "Tokenization method", "option_d": "Parsing technique", "correct_answer": "B", "explanation": "Attention allows models to focus on relevant parts of input when generating each output element."},
    {"topic": "NLP", "difficulty": "hard", "question_text": "What is BERT's main training objective?", "option_a": "Next sentence prediction only", "option_b": "Masked language modeling and next sentence", "option_c": "Translation", "option_d": "Summarization", "correct_answer": "B", "explanation": "BERT is trained on masked language modeling (predicting masked words) and next sentence prediction."},
    {"topic": "NLP", "difficulty": "hard", "question_text": "What is the difference between GRU and LSTM?", "option_a": "GRU has more gates", "option_b": "GRU has fewer gates, simpler architecture", "option_c": "They are identical", "option_d": "GRU is not for sequences", "correct_answer": "B", "explanation": "GRU has 2 gates (reset, update) vs LSTM's 3 (input, forget, output), making it simpler."},
    {"topic": "NLP", "difficulty": "hard", "question_text": "What is perplexity in language models?", "option_a": "Training speed", "option_b": "Measure of how well model predicts text", "option_c": "Number of parameters", "option_d": "Vocabulary size", "correct_answer": "B", "explanation": "Perplexity measures how surprised a model is by test data; lower is better for language models."},
    
    # ==================== LLMs ====================
    # Easy
    {"topic": "LLMs", "difficulty": "easy", "question_text": "What does LLM stand for?", "option_a": "Linear Learning Model", "option_b": "Large Language Model", "option_c": "Layered Logic Module", "option_d": "Linguistic Learning Machine", "correct_answer": "B", "explanation": "LLM stands for Large Language Model, trained on massive text data to understand and generate language."},
    {"topic": "LLMs", "difficulty": "easy", "question_text": "What is a prompt in LLMs?", "option_a": "Model weights", "option_b": "Input text given to the model", "option_c": "Training data", "option_d": "Output format", "correct_answer": "B", "explanation": "A prompt is the input text given to an LLM to generate a response or completion."},
    {"topic": "LLMs", "difficulty": "easy", "question_text": "What is GPT?", "option_a": "General Purpose Tokenizer", "option_b": "Generative Pre-trained Transformer", "option_c": "Gradient Processing Tool", "option_d": "Graph Pattern Tracker", "correct_answer": "B", "explanation": "GPT stands for Generative Pre-trained Transformer, a type of large language model."},
    {"topic": "LLMs", "difficulty": "easy", "question_text": "What is fine-tuning an LLM?", "option_a": "Building from scratch", "option_b": "Adapting a pre-trained model to specific tasks", "option_c": "Reducing model size", "option_d": "Creating embeddings", "correct_answer": "B", "explanation": "Fine-tuning adapts a pre-trained model to specific tasks using task-specific training data."},
    {"topic": "LLMs", "difficulty": "easy", "question_text": "What is the context window in LLMs?", "option_a": "Training duration", "option_b": "Maximum input text length", "option_c": "Number of layers", "option_d": "Vocabulary size", "correct_answer": "B", "explanation": "Context window is the maximum number of tokens an LLM can process in a single input."},
    
    # Medium
    {"topic": "LLMs", "difficulty": "medium", "question_text": "What is the transformer architecture based on?", "option_a": "Recurrence", "option_b": "Self-attention mechanism", "option_c": "Convolution", "option_d": "Decision trees", "correct_answer": "B", "explanation": "Transformers rely on self-attention to process sequences in parallel, replacing recurrence."},
    {"topic": "LLMs", "difficulty": "medium", "question_text": "What is zero-shot learning in LLMs?", "option_a": "Training from scratch", "option_b": "Performing tasks without examples", "option_c": "Using one example", "option_d": "Maximum training", "correct_answer": "B", "explanation": "Zero-shot learning enables models to perform tasks without any task-specific training examples."},
    {"topic": "LLMs", "difficulty": "medium", "question_text": "What is few-shot learning?", "option_a": "Minimal training data overall", "option_b": "Learning from a few examples in the prompt", "option_c": "Fast training", "option_d": "Reduced parameters", "correct_answer": "B", "explanation": "Few-shot learning uses a small number of examples in the prompt to guide model behavior."},
    {"topic": "LLMs", "difficulty": "medium", "question_text": "What is temperature in LLM generation?", "option_a": "Hardware temperature", "option_b": "Parameter controlling output randomness", "option_c": "Training speed", "option_d": "Context length", "correct_answer": "B", "explanation": "Temperature controls randomness in token selection; higher values = more diverse outputs."},
    {"topic": "LLMs", "difficulty": "medium", "question_text": "What is tokenization in LLMs typically based on?", "option_a": "Words only", "option_b": "Subword units (BPE, SentencePiece)", "option_c": "Characters only", "option_d": "Sentences", "correct_answer": "B", "explanation": "Modern LLMs use subword tokenization (BPE, SentencePiece) balancing vocabulary size and coverage."},
    {"topic": "LLMs", "difficulty": "medium", "question_text": "What is RLHF?", "option_a": "Random Learning High Frequency", "option_b": "Reinforcement Learning from Human Feedback", "option_c": "Recurrent Learning for Hierarchies", "option_d": "Regularized Linear Hidden Features", "correct_answer": "B", "explanation": "RLHF uses human preferences to fine-tune models, making outputs more helpful and aligned."},
    
    # Hard
    {"topic": "LLMs", "difficulty": "hard", "question_text": "What is the purpose of positional encoding?", "option_a": "Reducing model size", "option_b": "Providing sequence position information", "option_c": "Attention masking", "option_d": "Token embedding", "correct_answer": "B", "explanation": "Positional encodings provide position information since transformers don't have inherent order awareness."},
    {"topic": "LLMs", "difficulty": "hard", "question_text": "What is multi-head attention?", "option_a": "Single attention with multiple outputs", "option_b": "Multiple parallel attention operations", "option_c": "Attention on multiple sequences", "option_d": "Hierarchical attention", "correct_answer": "B", "explanation": "Multi-head attention runs multiple attention operations in parallel, capturing different relationships."},
    {"topic": "LLMs", "difficulty": "hard", "question_text": "What is LoRA in LLM fine-tuning?", "option_a": "Layer Optimization for Retrieval", "option_b": "Low-Rank Adaptation of weights", "option_c": "Learning Object Recognition", "option_d": "Length of Response Adjustment", "correct_answer": "B", "explanation": "LoRA adds small low-rank matrices for efficient fine-tuning without modifying original weights."},
    {"topic": "LLMs", "difficulty": "hard", "question_text": "What causes hallucination in LLMs?", "option_a": "Too much training data", "option_b": "Model generating plausible but false information", "option_c": "Insufficient parameters", "option_d": "High temperature only", "correct_answer": "B", "explanation": "Hallucinations occur when models generate confident but factually incorrect or fabricated content."},
    {"topic": "LLMs", "difficulty": "hard", "question_text": "What is chain-of-thought prompting?", "option_a": "Linking multiple models", "option_b": "Prompting model to show reasoning steps", "option_c": "Sequential training", "option_d": "Memory chaining", "correct_answer": "B", "explanation": "Chain-of-thought prompting encourages models to show step-by-step reasoning for better answers."},
    
    # ==================== GENERATIVE AI ====================
    # Easy
    {"topic": "Generative AI", "difficulty": "easy", "question_text": "What is generative AI?", "option_a": "AI that only classifies", "option_b": "AI that creates new content", "option_c": "AI that predicts numbers", "option_d": "AI that clusters data", "correct_answer": "B", "explanation": "Generative AI creates new content (text, images, audio) rather than just analyzing existing data."},
    {"topic": "Generative AI", "difficulty": "easy", "question_text": "What is an example of generative AI output?", "option_a": "Spam classification", "option_b": "AI-generated images", "option_c": "Stock price prediction", "option_d": "Customer segmentation", "correct_answer": "B", "explanation": "AI-generated images (like DALL-E outputs) are examples of generative AI creating new content."},
    {"topic": "Generative AI", "difficulty": "easy", "question_text": "What is DALL-E?", "option_a": "Text classifier", "option_b": "Image generation model", "option_c": "Speech recognition", "option_d": "Translation service", "correct_answer": "B", "explanation": "DALL-E is an AI model that generates images from text descriptions."},
    {"topic": "Generative AI", "difficulty": "easy", "question_text": "What is text-to-image generation?", "option_a": "Reading text from images", "option_b": "Creating images from text descriptions", "option_c": "Converting speech to text", "option_d": "Translating languages", "correct_answer": "B", "explanation": "Text-to-image generation creates images based on textual descriptions."},
    
    # Medium
    {"topic": "Generative AI", "difficulty": "medium", "question_text": "What are GANs?", "option_a": "Graph Analysis Networks", "option_b": "Generative Adversarial Networks", "option_c": "Gradient Averaging Networks", "option_d": "General Attention Networks", "correct_answer": "B", "explanation": "GANs consist of generator and discriminator networks competing to create realistic outputs."},
    {"topic": "Generative AI", "difficulty": "medium", "question_text": "What is the generator's role in GANs?", "option_a": "Classify real/fake", "option_b": "Create fake samples", "option_c": "Train discriminator", "option_d": "Preprocess data", "correct_answer": "B", "explanation": "The generator creates fake samples trying to fool the discriminator into thinking they're real."},
    {"topic": "Generative AI", "difficulty": "medium", "question_text": "What is a diffusion model?", "option_a": "Model that spreads data", "option_b": "Model that learns to denoise data", "option_c": "Classification model", "option_d": "Clustering model", "correct_answer": "B", "explanation": "Diffusion models learn to generate by reversing a gradual noising process."},
    {"topic": "Generative AI", "difficulty": "medium", "question_text": "What is Stable Diffusion?", "option_a": "Physics simulation", "option_b": "Text-to-image diffusion model", "option_c": "Chemical process model", "option_d": "Network stability tool", "correct_answer": "B", "explanation": "Stable Diffusion is a latent diffusion model for generating images from text prompts."},
    {"topic": "Generative AI", "difficulty": "medium", "question_text": "What is a VAE (Variational Autoencoder)?", "option_a": "Classification model", "option_b": "Generative model with latent space", "option_c": "Clustering algorithm", "option_d": "Regression model", "correct_answer": "B", "explanation": "VAEs encode data to latent space and decode to generate new samples."},
    {"topic": "Generative AI", "difficulty": "medium", "question_text": "What is prompt engineering?", "option_a": "Building hardware", "option_b": "Crafting effective inputs for AI models", "option_c": "Training from scratch", "option_d": "Database design", "correct_answer": "B", "explanation": "Prompt engineering involves crafting input text to get desired outputs from generative models."},
    
    # Hard
    {"topic": "Generative AI", "difficulty": "hard", "question_text": "What is mode collapse in GANs?", "option_a": "Generator fails to train", "option_b": "Generator produces limited variety", "option_c": "Discriminator wins", "option_d": "Memory overflow", "correct_answer": "B", "explanation": "Mode collapse occurs when the generator produces limited variations, not covering the full data distribution."},
    {"topic": "Generative AI", "difficulty": "hard", "question_text": "What is the latent space in generative models?", "option_a": "Output layer", "option_b": "Compressed representation space", "option_c": "Training data", "option_d": "Loss function", "correct_answer": "B", "explanation": "Latent space is a compressed representation where similar items are close together."},
    {"topic": "Generative AI", "difficulty": "hard", "question_text": "What is classifier-free guidance in diffusion?", "option_a": "Removing classifiers", "option_b": "Guiding generation without external classifier", "option_c": "Classification method", "option_d": "Training technique", "correct_answer": "B", "explanation": "Classifier-free guidance steers generation toward prompts without needing a separate classifier."},
    {"topic": "Generative AI", "difficulty": "hard", "question_text": "What is the CLIP model used for in image generation?", "option_a": "Image classification only", "option_b": "Connecting text and image representations", "option_c": "Audio processing", "option_d": "Video editing", "correct_answer": "B", "explanation": "CLIP learns to connect text and images in shared embedding space, guiding image generation."},
    
    # ==================== VERY HARD / EXPERT ====================
    # Very Hard
    {"topic": "Statistics", "difficulty": "very_hard", "question_text": "What is the difference between parametric and non-parametric tests?", "option_a": "Both assume normal distribution", "option_b": "Parametric assumes specific distributions, non-parametric doesn't", "option_c": "Non-parametric is always better", "option_d": "Parametric has no assumptions", "correct_answer": "B", "explanation": "Parametric tests assume data follows certain distributions; non-parametric tests make fewer assumptions."},
    {"topic": "Machine Learning", "difficulty": "very_hard", "question_text": "What is the PAC learning framework?", "option_a": "Parallel Algorithm Computing", "option_b": "Probably Approximately Correct learning", "option_c": "Pattern Analysis Classification", "option_d": "Predictive Accuracy Calculation", "correct_answer": "B", "explanation": "PAC learning formalizes the goal of learning a good enough hypothesis with high probability."},
    {"topic": "Deep Learning", "difficulty": "very_hard", "question_text": "What is the dying ReLU problem?", "option_a": "ReLU activation is too slow", "option_b": "Neurons stuck outputting zero permanently", "option_c": "Gradient explosion", "option_d": "Memory issues", "correct_answer": "B", "explanation": "Dying ReLU occurs when neurons always output 0 for negative inputs, never updating."},
    {"topic": "Machine Learning", "difficulty": "very_hard", "question_text": "What is the VC dimension?", "option_a": "Validation curve", "option_b": "Measure of model capacity/complexity", "option_c": "Variance component", "option_d": "Vector count", "correct_answer": "B", "explanation": "VC dimension measures the capacity of a model class, indicating what patterns it can learn."},
    {"topic": "Deep Learning", "difficulty": "very_hard", "question_text": "What is neural architecture search (NAS)?", "option_a": "Manual network design", "option_b": "Automated architecture optimization", "option_c": "Weight initialization", "option_d": "Hyperparameter tuning only", "correct_answer": "B", "explanation": "NAS automates the search for optimal neural network architectures using algorithms."},
    {"topic": "NLP", "difficulty": "very_hard", "question_text": "What is the difference between encoder and decoder in transformers?", "option_a": "Encoder generates, decoder understands", "option_b": "Encoder processes input, decoder generates output", "option_c": "They are identical", "option_d": "Decoder is for images only", "correct_answer": "B", "explanation": "Encoder processes input into representations; decoder generates output sequence from those representations."},
    {"topic": "LLMs", "difficulty": "very_hard", "question_text": "What is FlashAttention?", "option_a": "Faster inference only", "option_b": "Memory-efficient attention computation", "option_c": "Smaller model size", "option_d": "Faster tokenization", "correct_answer": "B", "explanation": "FlashAttention reduces memory usage and speeds up attention by avoiding materializing the full attention matrix."},
    {"topic": "Generative AI", "difficulty": "very_hard", "question_text": "What is score matching in diffusion models?", "option_a": "Evaluating output quality", "option_b": "Learning the gradient of data distribution", "option_c": "Comparing images", "option_d": "Classification metric", "correct_answer": "B", "explanation": "Score matching trains the model to estimate the score (gradient of log-density) of the data."},
    
    # Expert
    {"topic": "Statistics", "difficulty": "expert", "question_text": "What is the method of moments estimation?", "option_a": "Using sample moments to estimate parameters", "option_b": "Maximizing likelihood", "option_c": "Minimizing variance", "option_d": "Bayesian inference", "correct_answer": "A", "explanation": "Method of moments sets sample moments equal to theoretical moments to solve for parameters."},
    {"topic": "Machine Learning", "difficulty": "expert", "question_text": "What is the Rademacher complexity?", "option_a": "Training time measure", "option_b": "Measure of richness of function class", "option_c": "Accuracy metric", "option_d": "Memory usage", "correct_answer": "B", "explanation": "Rademacher complexity measures how well a function class can fit random noise, indicating capacity."},
    {"topic": "Deep Learning", "difficulty": "expert", "question_text": "What is the lottery ticket hypothesis?", "option_a": "Random initialization works best", "option_b": "Sparse subnetworks can match dense network performance", "option_c": "More parameters always better", "option_d": "Training is random", "correct_answer": "B", "explanation": "Lottery ticket hypothesis: dense networks contain sparse subnetworks that can train to similar performance."},
    {"topic": "Reinforcement Learning", "difficulty": "expert", "question_text": "What is the difference between on-policy and off-policy learning?", "option_a": "On-policy is faster", "option_b": "On-policy learns from current policy, off-policy from any", "option_c": "Off-policy needs more data", "option_d": "They are the same", "correct_answer": "B", "explanation": "On-policy learns from actions taken by current policy; off-policy can learn from any collected data."},
    {"topic": "LLMs", "difficulty": "expert", "question_text": "What is KV-cache in transformer inference?", "option_a": "Training optimization", "option_b": "Caching key-value pairs for faster generation", "option_c": "Model compression", "option_d": "Data preprocessing", "correct_answer": "B", "explanation": "KV-cache stores key and value matrices from previous tokens to avoid recomputation during generation."},
    {"topic": "Generative AI", "difficulty": "expert", "question_text": "What is the ELBO in VAEs?", "option_a": "Encoder Layer Batch Optimization", "option_b": "Evidence Lower Bound objective", "option_c": "Error Loss Base Output", "option_d": "Embedding Learning Base Objective", "correct_answer": "B", "explanation": "ELBO (Evidence Lower Bound) is the VAE training objective, combining reconstruction and KL terms."},
    {"topic": "Deep Learning", "difficulty": "expert", "question_text": "What is gradient checkpointing?", "option_a": "Saving gradients to disk", "option_b": "Recomputing activations to save memory", "option_c": "Gradient verification", "option_d": "Checkpoint averaging", "correct_answer": "B", "explanation": "Gradient checkpointing trades computation for memory by recomputing activations during backprop."},
    {"topic": "Machine Learning", "difficulty": "expert", "question_text": "What is the Johnson-Lindenstrauss lemma about?", "option_a": "Clustering bounds", "option_b": "Random projections preserve distances", "option_c": "Classification accuracy", "option_d": "Regression convergence", "correct_answer": "B", "explanation": "JL lemma states random projections can reduce dimensionality while approximately preserving distances."},
    {"topic": "NLP", "difficulty": "expert", "question_text": "What is the difference between autoregressive and autoencoding LMs?", "option_a": "Autoregressive is bidirectional", "option_b": "Autoregressive generates left-to-right, autoencoding sees full context", "option_c": "They are identical", "option_d": "Autoencoding generates text", "correct_answer": "B", "explanation": "Autoregressive (GPT) generates token by token; autoencoding (BERT) processes full input bidirectionally."},
    {"topic": "Reinforcement Learning", "difficulty": "expert", "question_text": "What is the difference between value and policy iteration?", "option_a": "Value iteration is slower", "option_b": "Value updates values, policy updates policy directly", "option_c": "Policy iteration uses neural networks", "option_d": "They solve different problems", "correct_answer": "B", "explanation": "Value iteration repeatedly updates value function; policy iteration alternates policy evaluation and improvement."},
]


def seed_questions():
    """Seed the database with questions"""
    db = SessionLocal()
    
    try:
        # Check if questions already exist
        existing_count = db.query(Question).count()
        if existing_count > 0:
            print(f"Database already has {existing_count} questions. Skipping seed.")
            return
        
        # Add all questions
        for q in QUESTIONS:
            question = Question(**q)
            db.add(question)
        
        db.commit()
        print(f"Successfully seeded {len(QUESTIONS)} questions!")
        
        # Print stats by difficulty
        for diff in ["easy", "medium", "hard", "very_hard", "expert"]:
            count = db.query(Question).filter(Question.difficulty == diff).count()
            print(f"  {diff}: {count} questions")
        
    except Exception as e:
        print(f"Error seeding questions: {e}")
        db.rollback()
    finally:
        db.close()


if __name__ == "__main__":
    seed_questions()
